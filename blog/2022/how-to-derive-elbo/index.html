<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>        
    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Marshal Sinaga | Some Notes on Deriving Evidence Lower Bound (ELBO)</title>
    <meta name="author" content="Marshal Arijona Sinaga" />
    <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
" />
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://gitcdn.xyz/repo/jwarby/jekyll-pygments-themes/master/github.css" />

    <!-- Styles -->
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22></text></svg>">
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://marshalarijona.github.io/blog/2022/how-to-derive-elbo/">

    <!-- Dark Mode -->
    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
  </head>

  <!-- Body -->
  <body class="fixed-top-nav">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="https://marshalarijona.github.io/">Marshal Sinaga</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">About</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">Blog</a>
              </li>

              <!-- Other pages-->
              <li class="nav-item ">
                <a class="nav-link" href="/notes/">Notes</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">Publications</a>
              </li>
              <!--
              <li class="nav-item dropdown ">
                <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus</a>
                <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown">
                  <a class="dropdown-item" href="/publications/">publications</a>
                  <div class="dropdown-divider"></div>
                  <a class="dropdown-item" href="/projects/">projects</a>
                </div>
              </li>
            -->

              <!-- Toogle theme mode -->
              <div class="toggle-container">
                <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </a>
              </div>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <!-- _layouts/post.html -->

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Some Notes on Deriving Evidence Lower Bound (ELBO)</h1>
    <p class="post-meta">February 8, 2022</p>
    <p class="post-tags">
      <a href="/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a>
        ·  
        <a href="/blog/tag/variational-inference">
          <i class="fas fa-hashtag fa-sm"></i> variational-inference</a>  
          <a href="/blog/tag/Kullback-Leibler-divergence">
          <i class="fas fa-hashtag fa-sm"></i> Kullback-Leibler-divergence</a>  
          
        ·  
        <a href="/blog/category/machine-learning-posts">
          <i class="fas fa-tag fa-sm"></i> machine-learning-posts</a>  
          

    </p>
  </header>

  <article class="post-content">
    <p>Bayesian inference approximation commonly relies on variational inference. This technique requires us to maximize evidence lower bound (ELBO). This article discusses three ways to derive ELBO. The outline is arranged as follows. First, we briefly highlight the motivation of variational inference. Subsequently, we provide three ways to derive ELBO. Finally, we end this article by a conclusion.</p>

<h2 id="background">Background</h2>
<p>Performing Bayesian inference requires us to compute posterior distribution. Inference can be thought as a process to quantify unknown variables given the observed variables. Let \(\mathcal{D} = \{x_i\}_{i=1}^{N}\) be a dataset contains \(N\) data points. We can view \(\mathcal{D}\) as the observed variables. Subsequently, we assume there is unobserved parameters \(\theta \in \Theta\) that provide explanations for \(\forall x \in \mathcal{X}\) in general. Inference aims to estimate \(\theta\) given the observations \(\mathcal{D}\). Performing inference under Bayesian theorem provides us a tool to quantify the uncertainty about \(\theta\). Therefore, Bayesian inference is treated as a probabilistic model. The core of Bayesian inference is to evaluate the posterior distribution \(p(\theta \vert \mathcal{D})\). This evaluation requires the likelihood \(p(\mathcal{D} \vert \theta)\), prior distribution \(p(\theta)\), and marginal distribution \(p(\mathcal{D})\). The likelihood tells the probability of \(\mathcal{D}\) given a certain \(\theta\). Meanwhile, prior \(p(\theta)\) provides our belief about \(\theta\) before we perform the inference. The third one is marginal distribution \(p(\mathcal{D})\) which represents the probability \(\mathcal{D}\) without the conditioning constraint. By applying Bayes’s theorem, we can write \(p(\theta \vert \mathcal{D})\) as:</p>

<p>\begin{equation}
\label{eq:posterior-distribution}
p(\theta \vert \mathcal{D}) = \frac{p(\mathcal{D}, \theta)}{p(\mathcal{D})} = \frac{p(\mathcal{D} \vert \theta) p(\theta)}{p(\mathcal{D})}
\end{equation}</p>

<p>Under the assumption that each data point \(x_i\) is i.i.d., we have \(p(\mathcal{D} \vert \theta) = \prod_{i=1}^{N} \, p(x_i \vert \theta)\). It seems like we are done with the problem. However, computing posterior distribution directly is not feasible.
<!--In practice, we compute $$\log p(\theta \vert \mathcal{D})$$ instead of $$p(\theta \vert \mathcal{D})$$ because the logarithm operation is numerically more stable. Aside from that, some properties of logarithm including monotonicity and product rule offers conveniences for the computation. --></p>

<p>Posterior distribution has a problem with computation intractability and we need variational inference to solve the issue. We obtain the marginal distribution \(p(\mathcal{D})\) by integrating \(p(\mathcal{D}, \theta)\) over all possible \(\theta\). Formally, we can write \(= \int p(\mathcal{D}, \theta) d\theta = \int p(\mathcal{D} | \theta) p(
\theta) d\theta\). Evaluating the distribution is often intractable. Intractablity can have two meanings:</p>
<ul>
  <li>The marginal distribution \(p(\mathcal{D})\) has no closed-form solution.</li>
  <li>The marginal distribution is computationally intractable (especially when \(x\) is high-dimensional). Look at <a href="https://arxiv.org/pdf/1601.00670.pdf" target="_blank" rel="noopener noreferrer">[Blei, 2016]</a> for more details</li>
</ul>

<p>We mitigate the intractability with the help of variational inference. Variational inference approximate the posterior \(p(\theta \vert \mathcal{D})\) by using a variational distribution \(q_{\phi} \in \mathcal{Q}\) parameterized by \(\phi\). Commonly, the variational distribution has a simpler form and relatively easy to compute. The goal is to find \(q^* \in \mathcal{Q}\) that is closest to \(p(\theta \vert \mathcal{D})\). For now, let’s just assume there is a function \(D[. \| .]\) that is able to measure the closeness between \(q_{\phi}(\theta)\) and \(p(\theta \vert \mathcal{D})\). The idea is to choose the parameters \(\phi\) from the parameter space \(\Phi\) that minimizes \(D[q_{\phi}(\theta) \| p(\theta \vert \mathcal{D})]\). Mathematically, we can write the objective of variational inference as follows:</p>

<p>\begin{equation}
\label{eq:variational-inference}
q^* = \underset{\phi \in \Phi}{\text{min}} \, D[q_{\phi}(\theta) | p(\theta \vert \mathcal{D})]
\end{equation}</p>

<p>Now our inference problem turns into an optimization problem. Figure below gives an illustration of variational inference. We start with an initialized distribution \(q\). Eventually, we obtain \(q^*\) through optimization process. By doing approximation, we trade some accuracy with a more efficient computation. But, how do we define \(D[. \| .]\) ?</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/variational-inference-480.webp"></source>
    <source media="(max-width: 800px)" srcset="/assets/img/variational-inference-800.webp"></source>
    <source media="(max-width: 1400px)" srcset="/assets/img/variational-inference-1400.webp"></source>
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/variational-inference.png">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    An illustration of variational inference.
</div>

<h2 id="deriving-elbo-by-minimizing-kl-divergence">Deriving ELBO by Minimizing KL-Divergence</h2>
<p>In this section, we introduce KL-divergence as a way to define \(D[. \| .]\). Based on KL-divergence, we derive ELBO, a more practical objective function of variational inference. Given two distributions \(p\) and \(q\), KL function \(KL[. \| .] : \mathcal{P} \times \mathcal{P} \rightarrow \mathbb{R}\) has the following form:
\begin{equation}
\label{eq:kl-divergence}
KL[p | q] = \mathbb{E}_{p(\theta)}\left[ \log \frac{p(\theta)}{q(\theta)} \right] = \int p(\theta) \log \frac{p(\theta)}{q(\theta)} d\theta
\end{equation}
Some properties of KL-divergence including:</p>
<ul>
  <li>KL-divergence is zero iff \(p=q\)</li>
  <li>KL-divergence is not symmetric, that is \(KL[p \| q] \neq KL[q \| p]\)</li>
  <li>KL-divergence satisfies \(KL[. \| .] \geq 0\)</li>
</ul>

<p>Having a minimum KL-divergence means that we can obtain a tight approximation of \(p(\theta \vert \mathcal{D})\). By applying the conditional probability theory and log properties on \(KL[q_{\phi}(\theta) \| p(\theta \vert \mathcal{D})]\), we obtain:</p>

\[\begin{eqnarray}
KL[q_{\phi}(\theta) \| p(\theta \vert \mathcal{D})] &amp;=&amp; \mathbb{E}_{q_{\phi}(\theta)}\left[\log \frac{q_{\phi}(\theta)}{p(\theta \vert \mathcal{D})} \right] \nonumber \\
&amp;=&amp; \mathbb{E}_{q_{\phi}(\theta)}\left[ \log \frac{q_{\phi}(\theta)p(\mathcal{D})}{p(\mathcal{D}, \theta)} \right] \nonumber \\
&amp;=&amp; \mathbb{E}_{q_{\phi}(\theta)}[\log p(\mathcal{D})] + \mathbb{E}_{q_{\phi}(\theta)}\left[\log \frac{q_{\phi}(\theta)}{p(\mathcal{D}, \theta)}\right] \nonumber \\
&amp;=&amp;  \log p(\mathcal{D}) - \mathbb{E}_{q_{\phi}(\theta)} \left[ \log \frac{p(\mathcal{D}, \theta)}{q_{\phi}(\theta)} \right] \label{eq:elbo-kl}
\end{eqnarray}\]

<p>From Equation \eqref{eq:elbo-kl}, we can minimize \(KL[q_{\phi}(\theta) \| p(\theta \vert \mathcal{D})]\) by maximizing \(\mathcal{L} = \mathbb{E}_{q(\theta)} \left[ \log \frac{p(\mathcal{D}, \theta)}{q(\theta)} \right]\). The later term is called evidence lower bound (ELBO). \(KL[q_{\phi} \| p(\theta \vert \mathcal{D})]\) tells us the gap between \(\log p(\mathcal{D})\) and \(\mathcal{L}\). When \(KL[q_{\phi}(\theta) \| p(\theta \vert \mathcal{D})] = 0\) then \(\log p(\mathcal{D}) = \mathcal{L}\). Discarding the KL-term from \eqref{eq:elbo-kl} gives us an inequality:</p>

<p>\begin{equation}
\log p(\mathcal{D}) \geq \mathbb{E}_{q(\theta)} \left[ \log \frac{p(\mathcal{D}, \theta)}{q(\theta)} \right]
\end{equation}</p>

<p>Eventually, our optimization problem turns into ELBO maximization:</p>

<p>\begin{equation}
q^* = \underset{\phi \in \Phi}{\text{max}} \, \mathbb{E}_{q(\theta)} \left[ \log \frac{p(\mathcal{D}, \theta)}{q(\theta)} \right]
\end{equation}</p>

<p>It turns out that deriving ELBO from KL-divergence is not the only way. In the next two sections, we elaborate different approaches to derive ELBO.</p>

<h2 id="deriving-elbo-by-using-jensens-inequality">Deriving ELBO by Using Jensen’s Inequality</h2>
<p>In this section, we show how to derive ELBO by using the definition of \(\log p(\mathcal{D})\) and Jensen’s inequality. Recall that we can obtain marginal distribution \(p(\mathcal{D})\) by marginalizing the joint distribution \(p(\theta, \mathcal{D})\) over \(\theta\). Now, let us define \(\log p(\mathcal{D})\) by involving the variational distribution \(q_{\phi}(\theta)\).</p>

\[\begin{eqnarray}
\log p(\mathcal{D}) &amp;=&amp; \log \int p(\mathcal{D}, \theta) d\theta \nonumber \\
&amp;=&amp; \log \int \frac{p(\mathcal{D}, \theta) q(\theta)}{q(\theta)} d\theta \nonumber \\
&amp;=&amp; \log \mathbb{E}_{q(\theta)} \left[ \frac{p(\mathcal{D}, \theta)}{q(\theta)} \right] \label{eq:convex-elbo} \\
\end{eqnarray}\]

<p>In order to derive ELBO, we rely on Jensen’s inequality. In the context of probability theory, Jensen inequality states that:</p>

<p><em>If \(X\) is a random variable and \(f:X \rightarrow \mathbb{R}\) is a convex function, then it satisfies \(\mathbb{E}[f(X)] \leq f(\mathbb{E}[X])\).</em></p>

<p>Now, convince yourself that Equation \eqref{eq:convex-elbo} is a convex function. Therefore, it satisfies:</p>

<p>\begin{equation}
\label{eq:elbo-jensen} 
\log p(\mathcal{D}) \geq \mathbb{E}_{q(\theta)}\left[ \frac{p(\mathcal{D}, \theta)}{q(\theta)} \right] \nonumber
\end{equation}</p>

<h2 id="alternative-derivation">Alternative Derivation</h2>
<p>In this section, we derive the last approach to obtain ELBO. This approach is based on the Bayes’s theorem. Observe that we can rearrange Equation \eqref{eq:posterior-distribution} as follows:</p>

<p>\begin{equation}
p(\mathcal{D}) = \frac {p(\theta \vert \mathcal{D}) p(\theta)}{p(\theta \vert \mathcal{D})} \nonumber
\end{equation}</p>

<p>This equation holds for any \(\theta\). Subsequently, let us take the log for both sides:</p>

<p>\begin{equation}
\log p(\mathcal{D}) = \log p(\mathcal{D}, \theta) - \log p(\theta \vert \mathcal{D}) \nonumber
\end{equation}</p>

<p>Now, let us include the variational distribution \(q_{\phi}\) without affect the equation above:</p>

\[\begin{eqnarray}
\log p(\mathcal{D}) &amp;=&amp; \log p(\mathcal{D}, \theta) - \log p(\theta \vert \mathcal{D}) + \log q_{\phi}(\theta) - \log q(\phi)(\theta) \nonumber \\
&amp;=&amp; \log p(\mathcal{D}, \theta) - \log \frac{p(\theta \vert \mathcal{D})}{q_{\phi}(\theta)} - \log q_{\phi}(\theta) \nonumber
\end{eqnarray}\]

<p>Recall that \(\log a \leq a - 1 \leftrightarrow - \log a \geq 1 - a\) for \(a \in \mathbb{R}^{+}\). Using this inequality, we have:</p>

\[\begin{eqnarray}
	\log p(\mathcal{D}) &amp;=&amp; \log p(\mathcal{D}, \theta) - \log \frac{p(\theta \vert \mathcal{D})}{q_{\phi}(\theta)} - \log q_{\phi}(\theta) \nonumber \\
	&amp;\geq&amp; \log p(\mathcal{D}, \theta) - \log q_{\phi}(\theta) + 1 - \frac{p(\theta \vert \mathcal{D})}{q_{\phi}(\theta)} \nonumber
\end{eqnarray}\]

<p>Since it is true for all \(\theta\), then it is also true under expectation. Therefore:</p>

\[\begin{eqnarray}
\log p(\mathcal{D)} &amp;\geq&amp; \int q_{\phi}(\theta) (\log p(\mathcal{D}, \theta) - \log q_{\phi}(\theta) + 1 - \frac{p(\theta \vert \mathcal{D})}{q_{\phi}(\theta)}) d\theta \nonumber \\
&amp;=&amp; \int q_{\phi}(\theta) \log \frac{p(\mathcal{D}, \theta)}{q_{\phi}(\theta)} d\theta + 1 - \int q_{\phi}(\theta) \frac{p(\theta \vert \mathcal{D})}{q_{\phi}(\theta)} d\theta \nonumber \\
 &amp;=&amp; \int q_{\phi}(\theta) \log \frac{p(\mathcal{D}, \theta)}{q_{\phi}(\theta)} d\theta + 1 - p(\theta \vert \mathcal{D}) d\theta \nonumber \\
 &amp;=&amp; \int q_{\phi}(\theta) \log \frac{p(\mathcal{D}, \theta)}{q_{\phi}(\theta)} d\theta + 1 - 1 \nonumber \\
 &amp;=&amp; \mathcal{L}
\end{eqnarray}\]

<p>Finally, we obtain ELBO without using KL-divergence.</p>

<h2 id="conclusion">Conclusion</h2>
<p>In conclusion, variational inference is introduced to overcome the intractability of computation posterior. In this article we elaborate 3 different ways to derive ELBO. The first approach is based on KL divergence between variational distribution \(q_{\phi}(\theta)\) and posterior \(p(\mathcal{D} \vert \theta)\). The second approach relies on marginalization of joint distribution and Jensen inequality. Finally, we show that ELBO can be derived based on the original Bayes theorem.</p>

<h6 id="references"><strong>References</strong></h6>

<ul>
  <li>Bishop, C. M., &amp; Nasrabadi, N. M. (2006). Pattern recognition and machine learning (Vol. 4, No. 4, p. 738). New York: springer.</li>
  <li>Blei, D. M., Kucukelbir, A., &amp; McAuliffe, J. D. (2017). Variational inference: A review for statisticians. Journal of the American statistical Association, 112(518), 859-877.</li>
  <li>Adams, R. (2020) The ELBO without Jensen, Kullback, or Leibler. Laboratory for Intelligent Probabilistic Systems, Princeton University, Department of Computer Science, https://lips.cs.princeton.edu/the-elbo-without-jensen-or-kl/.</li>
</ul>

<!-- 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 -->

  </article>

</div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2024 Marshal Arijona Sinaga. Powered by <a href="http://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Mansory & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/mansory.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
  </body>
</html>

