<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>        
    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Marshal Sinaga | Probability Improvement and Expected Improvement Acquisition Function</title>
    <meta name="author" content="Marshal Arijona Sinaga" />
    <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
" />
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://gitcdn.xyz/repo/jwarby/jekyll-pygments-themes/master/github.css" />

    <!-- Styles -->
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22></text></svg>">
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://marshalarijona.github.io/blog/2023/expected-improvement/">

    <!-- Dark Mode -->
    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
  </head>

  <!-- Body -->
  <body class="fixed-top-nav">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="https://marshalarijona.github.io/">Marshal Sinaga</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">About</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">Blog</a>
              </li>

              <!-- Other pages-->
              <li class="nav-item ">
                <a class="nav-link" href="/notes/">Notes</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">Publications</a>
              </li>
              <!--
              <li class="nav-item dropdown ">
                <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus</a>
                <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown">
                  <a class="dropdown-item" href="/publications/">publications</a>
                  <div class="dropdown-divider"></div>
                  <a class="dropdown-item" href="/projects/">projects</a>
                </div>
              </li>
            -->

              <!-- Toogle theme mode -->
              <div class="toggle-container">
                <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </a>
              </div>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <!-- _layouts/post.html -->

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Probability Improvement and Expected Improvement Acquisition Function</h1>
    <p class="post-meta">October 5, 2023</p>
    <p class="post-tags">
      <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>
        ·  
        <a href="/blog/tag/acquisition-function">
          <i class="fas fa-hashtag fa-sm"></i> acquisition-function</a>  
          <a href="/blog/tag/Bayesian-optimization">
          <i class="fas fa-hashtag fa-sm"></i> Bayesian-optimization</a>  
          
        ·  
        <a href="/blog/category/machine-learning-posts">
          <i class="fas fa-tag fa-sm"></i> machine-learning-posts</a>  
          

    </p>
  </header>

  <article class="post-content">
    <p>This post assumes the reader has a prequisite knowledge about Bayesian optimization. Recall that Bayesian optimization is a zero-th order optimization method which aims to find the optimum \(x^\ast\) of a black-box function \(f: \mathcal{X} \rightarrow \mathbb{R}\). Bayesian optimization requires two important components, that is the surrogate model and the acquisition function. Due to black-box nature of \(f\), we introduce a surrogate function and instead perform the optimization w.r.t. this new function. In common settings, we utilize a Gaussian process (GP) as our surrogate model. The motivation comes from the Bayesian philosophy where we start with a belief and iteratively update it as we encounter data which explains the true distribution called the posterior distribution.</p>

<p>This post is focused on the second component, that is the acquisition function. This function governs the next input \(x_t\) to be evaluated, with \(t\) denotes the particular time step of the acquisition. The acquired data has the optimum statistical properties w.r.t. our surrogate model, i.e. the expectation, entropy, etc. Now, let us narrowing our scope again into two specific acquisition functions, that is the probability improvement and the expected improvement.</p>

<h3 id="probability-improvement">Probability Improvement</h3>

<p>We first give the definition of improvement. Given the incummbent best value \(x^\ast \in \mathcal{X}\) and an arbitrary input \(x \in \mathcal{X}\), we define the improvement \(I(x)\) as</p>

\[\begin{eqnarray}
I(x) = \max(f(x) - f(x^\ast), 0)
\end{eqnarray}\]

<p>Note that we abuse the notation \(x^\ast\) for a while as previously \(x^\ast\) denotes the global optimum of the function \(f\). It is obvious that \(I(x) \in [0, +\infty] \quad \forall x \in \mathcal{X}\) and \(x = x^\ast \rightarrow I(x) = 0\). Recall that we employ GP as the proxy of \(f\). For a particular data \(x\), the function value \(f(x)\) follows a Gaussian distribution</p>

\[f(x) \sim \mathcal{N}(\mu(x), \sigma^2(x))\]

<p>Commonly we perform reparameterization-trick to draw samples from \(f(x)\). First, we introduce a random variable \(z\) drawn from a standard normal distribution \(\mathcal{N}(0, 1)\). It is known that drawing samples from such distribution is relatively easy. Leveraging this random variable, we obtain a new sample \(f(x) = \mu(x) + \sigma(x) z\). Substituting the new definition will give us</p>

\[I(x) = \max(f(x) - f(x^\ast)) = \max(\mu(x) + \sigma(x)z - f(x^\ast), 0) \quad z \sim \mathcal{N}(0, 1)\]

<p>The probability improvement evaluates how likely the candidate \(x\) gives us a positive improvement. Recall that we evaluate the probability w.r.t. \(\mathcal{N}(\mu(x), \sigma^2(x))\). Mathematically, we can write \(\mathrm{PI}(x)\) as</p>

\[\mathrm{PI}(x) = p(I(x) &gt; 0) \iff p(f(x) &gt; f(x^\ast))\]

<p>By applying the additive and constant scaling properties of normal distribution, we obtain the following analytical form</p>

\[\mathrm{PI}(x) = 1 - \Phi(z_0) = \Phi(- z_0) = \Phi\left( \frac{\mu(x) - f(x^\ast)}{\sigma(x)} \right)\]

<p>with \(\Phi(z) \triangleq \mathrm{CDF}(z)\) and \(z_0 = \frac{f(x^\ast) - \mu(x)}{\sigma(x)}\)</p>

<h3 id="expected-improvement">Expected Improvement</h3>
<p>Unlike the probability improvement, the expected improvement (EI) (as the name suggests) aims to evaluate the expected value of \(I(x)\) over \(f\). Intuitively, this criterion evaluates the average magnitude of the improvement.</p>

\[\begin{eqnarray}
\mathrm{EI}(x) \triangleq \mathbb{E}[I(x)] = \int_{- \infty}^\infty I(x) \phi(z)
\end{eqnarray}\]

<p>Substituting the definition of probability improvement, we then obtain</p>

\[\mathrm{EI} = \int_{- \infty}^\infty I(x) \phi(z) = \int_{- \infty}^\infty \max(f(x) - f(x^\ast), 0) \phi(z) dz\]

<p>In order to compute the integral, we need to get rid of the \(\max\) operator. First, we decompose the integral into two parts. The first part is where \(I(x) \leq 0\) and the later part is where \(I(x) &gt; 0\). To set the bound for each integral, recall that we can perform the reparameterization trick to rewerite \(f(x)\), that is \(f(x) = f(x^\ast) \rightarrow \mu + \sigma z = f(x^\ast) \rightarrow z_0 = \frac{f(x^\ast) - \mu}{\sigma}\). Thus, we can write \(\mathrm{EI}(x)\) as</p>

\[\mathrm{EI}(x) = \int_{- \infty}^{z_0} I(x) \phi(z) \, dz + \int_{z_0}^\infty I(x) \phi(z) \, dz\]

<p>Observe that the first term vanishes to \(0\) since \(\forall z \leq z_0\) we have \(I(x) = 0\). Therefore, we only need to evaluate the second part of the integral.</p>

\[\begin{aligned}
\mathrm{EI}(x) &amp;= \int_{z_0}^\infty \max(f(x) - f(x^\ast), 0) \phi(z) \, dz = \int_{z_0}^\infty \mu(x) + \sigma(x) z - f(x^\ast) \phi(z) \, dz \\
&amp;= \int_{z_0}^\infty (\mu - f(x^\ast)) \phi(z) dz + \int_{z_0}^\infty \sigma z \frac{1}{\sqrt{2 \pi}} \exp \left( \frac{-1}{2} z^2 \right ) dz \\
&amp;= (\mu - f(x^\ast)) \int_{z_0}^\infty \phi(z) dz + \frac{\sigma}{\sqrt{2 \pi}} \int_{z_0}^\infty z \exp \left( \frac{-1}{2} z^2 \right) dz \\
&amp;= (\mu - f(x^\ast)) (1 - \Phi(z_0)) - \int_{z_0}^\infty \left( \exp \left( \frac{-1}{2} z^2 \right) \right)^\prime dz \\
&amp;= (\mu - f(x^\ast)) (1 - \Phi(z_0)) - \frac{\sigma}{\sqrt{2 \pi}} \left[ \exp\left(\frac{-1}{2} z^2\right) \right]_{z_0}^\infty \\
&amp;= (\mu - f(x^\ast)) (1 - \Phi(z_0)) + \sigma \phi(z_0) \\
&amp;= (\mu - f(x^\ast)) \Phi\left( \frac{\mu - f(x^\ast)}{\sigma} \right) + \sigma \phi\left( \frac{\mu - f(x^\ast)}{\sigma} \right) 
\end{aligned}\]

<p>The last row comes from the fact that the normal density is symmetric, i.e., \(\phi(z_0) = \phi(- z_0)\). \(\mathrm{EI}(x)\) takes high value when \(\mu &gt; f(x^\ast)\) As a side note, \(\mathrm{EI}\) requires the uncertainty \(\sigma &gt; 0\) since \(\sigma = 0 \rightarrow \mathrm{EI}(x) = 0\). Finally, we intoroduce a hyperparameter \(\xi\) which control the degree of exploration</p>

\[\begin{eqnarray}
\mathrm{EI}(x; \xi) = (\mu - f(x^\ast) - \xi) \Phi\left( \frac{\mu - f(x^\ast) - \xi}{\sigma} \right) + \sigma \phi\left( \frac{\mu - f(x^\ast) - \xi}{\sigma} \right) 
\end{eqnarray}\]

<p>Note that \(\xi = 0 \rightarrow \mathrm{EI}(x; \xi) = \mathrm{EI}(x)\)</p>

<h6 id="references"><strong>References</strong></h6>

<ul>
  <li>Kamperis, S. (2021) Acquisition functions in Bayesian Optimization, https://ekamperi.github.io/machine%20learning/2021/06/11/acquisition-functions.html.</li>
</ul>

  </article>

</div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2024 Marshal Arijona Sinaga. Powered by <a href="http://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Mansory & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/mansory.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
  </body>
</html>

