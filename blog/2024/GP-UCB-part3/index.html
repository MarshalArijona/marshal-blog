<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>        
    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Marshal Sinaga | A Deep Dive into the Regret Bound of GP-UCB Optimization (Part III)</title>
    <meta name="author" content="Marshal Arijona Sinaga" />
    <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
" />
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://gitcdn.xyz/repo/jwarby/jekyll-pygments-themes/master/github.css" />

    <!-- Styles -->
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22></text></svg>">
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://marshalarijona.github.io/blog/2024/GP-UCB-part3/">

    <!-- Dark Mode -->
    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
  </head>

  <!-- Body -->
  <body class="fixed-top-nav">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="https://marshalarijona.github.io/">Marshal Sinaga</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">About</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">Blog</a>
              </li>

              <!-- Other pages-->
              <li class="nav-item ">
                <a class="nav-link" href="/notes/">Notes</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">Publications</a>
              </li>
              <!--
              <li class="nav-item dropdown ">
                <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus</a>
                <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown">
                  <a class="dropdown-item" href="/publications/">publications</a>
                  <div class="dropdown-divider"></div>
                  <a class="dropdown-item" href="/projects/">projects</a>
                </div>
              </li>
            -->

              <!-- Toogle theme mode -->
              <div class="toggle-container">
                <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </a>
              </div>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <!-- _layouts/post.html -->

<div class="post">

  <header class="post-header">
    <h1 class="post-title">A Deep Dive into the Regret Bound of GP-UCB Optimization (Part III)</h1>
    <p class="post-meta">June 18, 2024</p>
    <p class="post-tags">
      <a href="/blog/2024"> <i class="fas fa-calendar fa-sm"></i> 2024 </a>
        ·  
        <a href="/blog/tag/Bayesian-optimization">
          <i class="fas fa-hashtag fa-sm"></i> Bayesian-optimization</a>  
          
        ·  
        <a href="/blog/category/machine-learning-posts">
          <i class="fas fa-tag fa-sm"></i> machine-learning-posts</a>  
          

    </p>
  </header>

  <article class="post-content">
    <p>Tervetuloa! In this final part of the blog series (see <a href="https://marshalarijona.github.io/blog/2024/GP-UCB/">Part I</a> and <a href="https://marshalarijona.github.io/blog/2024/GP-UCB-part2/">Part II</a>), we are going to derive the bound of the quantity \(\gamma_T\) for practical classes of kernels.</p>

<h2 id="submodularity-and-greedy-maximization">Submodularity and Greedy maximization</h2>

<p><strong>Theorem 4</strong> <em>For any \(T \in \mathbb{N}\) and any \(T_\ast = 1, \dots, T:\)</em></p>

<p>\begin{equation}
\gamma_T \leq \mathcal{O}(\sigma^{-2} [B(T_\ast) T + T_\ast (\log n_T T)]) \nonumber
\end{equation}</p>

<p><em>where \(n_T = \sum_{t = 1}^{\vert D \vert} \hat{\lambda}_t\) and \(B(T_\ast) = \sum_{t = T_\ast + 1}^{\vert D \vert} \hat{\lambda}_t\)</em></p>

<hr>

<p><strong>Proof:</strong></p>

<p>The sketch of the proof is as follows:</p>
<ul>
  <li>First, note that the information gain \(I(\mathbf{y}_A; \mathbf{f}_A)\) is a submodular function. Therefore, \(\gamma_T\) can be bounded via greedy maximization.</li>
  <li>Subsequently, we utilize the discretization \(D_t \subset D\) with \(n_T = \vert D_T \vert = T^\tau\) with nearest neighbor distance \(o(1)\). We consider the kernel matrix \(\mathbf{K}_{D_T} \in \mathbb{R}^{n_T \times n_T}\) and bound \(\gamma_T\) by an expression involving the eigenvalues \(\{ \hat{\lambda}_t \}\) of this matrix.</li>
  <li>Finally, we obtain the bound of this empirical expression in terms of the kernel operator eigenvalues of \(k\).</li>
</ul>

<h4 id="greedy-maximization-and-discretization">Greedy Maximization and Discretization</h4>

<p><strong>Assumptions.</strong> Fix \(T \in \mathbb{N}\) and assume there exists a discretization \(D_T \subset D, n_T = \vert D_T \vert\) on the order of \(T^\tau\) s.t.</p>

\[\begin{eqnarray}
\forall x \in D \exists [x]_T \in D_T : \Vert x - [x]_T \Vert = \mathcal{O}(T^{- \tau / d}) \label{eq:discretization}
\end{eqnarray}\]

<p>We restrict the information gain to subsets \(A \subset D_T\):</p>

\[\begin{eqnarray}
\tilde{\gamma}_T = \max_{A \subset D_T, \vert A \vert = T} I(\mathbf{y}_A; \mathbf{f}_A) \nonumber
\end{eqnarray}\]

<hr>

<p><strong>Lemma 7.4</strong> <em>Under the assumptions of <strong>Theorem 2</strong>, the information gain \(F_T(\{ x_t \}) = 1 / 2 \log \vert \mathbf{I} + \sigma^{-2}  \vert\) is uniformly Lipschitz-continuous in each component \(x_t \in D\).</em></p>

<p><strong>Proof:</strong></p>

<p><strong>Theorem 2</strong> implies that the kernel \(k\) is continuously differentiable. The result follows from the fact that \(F_T(\{ x_t \})\) is continuously differentiable in the kernel matrix \(\mathbf{K}_{\{x_t\}}\).</p>

<hr>

<p><strong>Lemma 7.5</strong> <em>Let \(D_T\) be a discretization of \(D\) such that \eqref{eq:discretization} holds. Under the assumption of <strong>Theorem 2</strong>, we have that</em></p>

<p>\begin{equation}
0 \leq \gamma_T - \tilde{\gamma}_T = \mathcal{O}(T^{1 - \tau / d}) \nonumber
\end{equation}</p>

<p><strong>Proof:</strong></p>

<p>Fix \(T \in \mathbb{N}\). Let \(A = \{x_1, \dots, x_T\}\) be an optimum maximizer for \(\gamma_T\). Let \([A]_T = {[x_t]_T}_{t = 1}^T\). Then,</p>

\[\begin{eqnarray}
0 \leq \gamma_T - \tilde{\gamma}_T \leq \gamma_T - I(\mathbf{y}_{[A]_T}; \mathbf{f}_{[A]_T}) = F_T(A) - F_T([A]_T) \nonumber
\end{eqnarray}\]

<p>By <strong>Lemma 7.4</strong>, \(F_T\) is uniformly Lipschitz-continuous in each component, s.t. \(\vert \gamma_T  - I(\mathbf{y}_{[A]_T}; \mathbf{f}_{[A]_T}) \vert = \mathcal{O}(T \max_{t} \Vert x_t - [x_t]_T \Vert) = \mathcal{O}(T^{1 - \tau / d})\) by \eqref{eq:discretization} and the mean value theorem.</p>

<hr>

<p>With the following lemma, we obtain the upper-bound of \(\tilde{\gamma}_T\).</p>

<p><strong>Lemma 7.6</strong> <em>For any \(T \geq 1\), we have that</em></p>

\[\begin{eqnarray}
\hat{\gamma}_T \leq \frac{1 / 2}{1 - e^{-1}} \max_{m_1, \dots, m_T} \sum_{t = 1}^T \log(1 + \sigma^{-2} m_t \hat{\lambda}_t) \nonumber
\end{eqnarray}\]

<hr>

<h4 id="empirical-to-process-eigenvalues">Empirical to Process Eigenvalues</h4>

<ul>
  <li>Let \(\mu(x) = \mathcal{V}(D)^{-1} I_{x \in D}\) be the uniform distribution on \(D\), \(\mathcal{V}(D) = \int_{x \in D} dx\) and assume that \(k\) is continuous.</li>
  <li>Assuming \(k(x, x) = 1\), we have \(\int k(x, x) \mu(x) dx = 1\), so that \(k\) is Hilbert-Scmidt on \(L_2(\mu)\).</li>
  <li>Mercer’s theorem: corresponding kernel operator has a discrete eigenspectrum \(\{(\lambda_s, \phi_s(.))\}\) and</li>
</ul>

\[\begin{eqnarray}
k(x, x^\prime) = \sum_{s \geq 1} \lambda_s \phi_s(x) \phi_s(x^\prime) \nonumber
\end{eqnarray}\]

<p>where \(\lambda_1 \geq \lambda_2 \geq \dots \geq 0, \mathbb{E}_\mu[\phi_s(x) \phi_t(x)] = \delta_{s, t}\).</p>

<hr>
<p>The following lemma determines the sizes \(n_T\) for which the discretizations above exist.</p>

<p><strong>Lemma 7.7</strong> <em>Fix \(T \in \mathbb{N}, \delta &gt; 0\) and \(\epsilon &gt; 0\). There exists a discretization \(D_T \subset D\) of size</em></p>

<p>\begin{equation}
n_T = \mathcal{V}(D) (\epsilon / \sqrt{d})^{- d}[\log (1 / \delta) + d \log(\sqrt{d} / \epsilon) + \log \mathcal{V}(D)] \nonumber
\end{equation}</p>

<p><em>which fulfils the following requirements:</em></p>

<ul>
  <li>
    <p><em>\(\epsilon\)-denseness: For any \(x \in D\), there exists \([x]_T \in D_T\) such that \(\Vert x - [x]_T \Vert \leq \epsilon\).</em></p>
  </li>
  <li>
    <p><em>If \(\mathrm{spec}(\mathbf{K}_{D_T}) = \{\hat{\lambda}_1 \geq \hat{\lambda}_2  \geq \dots \}\), then for any \(T_\ast = 1, \dots, n_T\):</em></p>
  </li>
</ul>

\[\begin{eqnarray}
n_T^{-1} \sum_{t = 1}^{T_\ast} \hat{\lambda}_t \geq \sum_{t = 1}^{T_{\ast}} \lambda_t - \delta \nonumber
\end{eqnarray}\]

<hr>

<p>The following lemma is equivalent to <strong>Theorem 4</strong> in the context where this lemma is a direct consequence of <strong>Lemma 7.6</strong>.</p>

<p><strong>Lemma 7.8</strong> <em>Let \(D_T\) be some discretization of \(D, n_T = \vert D_T \vert\). For any \(T_\ast = 1, \dots, \min\{T, n_T\}\) :</em></p>

\[\begin{eqnarray}
\tilde{\gamma}_T \leq \frac{1 / 2}{1 - e^{-1}} \max_{r = 1, \dots, T} \left( T_\ast \log (r n_T / \sigma^2) + (T - r) \sigma^{-2} \sum_{t = T_\ast + 1}^{n_T} \hat{\lambda}_t \right) \nonumber
\end{eqnarray}\]

<p><strong>Proof:</strong></p>

<p>Split the right hand side in <strong>Lemma 7.6</strong> at \(t = T_\ast\). Let \(r = \sum_{t \leq T_\ast} m_t\).</p>

<ul>
  <li>For \(t \leq T_\ast\): \(\log (1 + m_t \hat{\lambda}_t / \sigma^2) \leq \log (r n_T / \sigma^2)\), since \(\hat{\lambda}_t \leq n_T\).</li>
  <li>For \(t &gt; T_\ast\): \(\log (1 + m_t \hat{\lambda}_t / \sigma^2) \leq (T - r) \hat{\lambda}_t / \sigma^2\).</li>
</ul>

<hr>

<p>The following theorem is responsible for obtaining bounds on \(\gamma_T\) for a particular kernel \(k\), given that tail bounds on \(B_k(T_\ast) = \sum_{s &gt; T_\ast} \lambda_s\) are known.</p>

<p><strong>Theorem 8</strong> <em>Suppose that \(D \subset \mathbb{R}^d\) is compact, and \(k(x, x^\prime)\) is a covariance function for which the additional assumption of <strong>Theorem 2</strong> holds. Moreover, let \(B_k(T_\ast) = \sum_{s &gt; T_\ast} \lambda_s\), where \(\{\lambda_s\}\) is the operator spectrum of \(k\)  with respect to the uniform distribution over \(D\). Pick \(\tau &gt; 0\), and let \(n_T = C_4 T^\tau (\log T)\) with \(C_4 = 2 \mathcal{V}(D)(2 \tau + 1)\). Then, the following bound holds:</em></p>

\[\begin{eqnarray}
\gamma_T \leq \frac{1 / 2}{1 - e^{-1}} \max_{r = 1, \dots, T} \left(T_\ast \log (r n_T / \sigma^2) + C_4 \sigma^{-2} (1 - r / T) (\log T) (T^{\tau + 1} B_k(T_\ast) + 1) \right) + \mathcal{O}(T^{1 - \tau / d}) \nonumber
\end{eqnarray}\]

<p><em>for any \(T_\ast \in \{1, \dots, n_T \}\).</em></p>

<p><strong>Proof:</strong></p>

<p>Let \(\epsilon = d^{1 / 2} T^{- \tau / d}\) and \(\delta = T^{- (\tau + 1)}\). <strong>Lemma 7.7</strong> provides the existence of a discretization \(D_T\) of size \(n_T\) which is \(\epsilon-\)dense and for which \(n_T^{-1} \sum_{t = 1}^{T_\ast} \hat{\lambda}_t \geq \sum_{t = 1}^{T_{\ast}} \lambda_t - \delta\). Since \(n_T^{-1} \sum_{t = 1}^{n_T} \hat{\lambda}_t = 1 = \sum_{t \geq 1} \hat{\lambda} \lambda_t\), then \(\). The statement follows by using <strong>Lemma 7.8</strong> with these bounds, and finally employing <strong>Lemma 7.5</strong>.</p>

<h2 id="bounds-for-kernels">Bounds for Kernels</h2>

<p>Next, we bound \(\gamma_T\) based on <strong>Theorem 8</strong> for a range of commonly used kernel functions: linear kernel, squared exponential kernel, and Matérn kernels. The results imply sublinear regret bound for GP-UCB in all cases.</p>

<p><strong>Linear Kernel.</strong> GP with this kernel corresponds to random linear function \(f(x) = w^\top x, \quad w \sim \mathcal{N}(\mathbf{0}, \mathbf{I})\)</p>

\[\begin{eqnarray}
k(x, x^\prime) = x^\top x^\prime \nonumber
\end{eqnarray}\]

<p><strong>Squared exponential kernel.</strong> Sample functions are differentiable to any order almost surely.</p>

\[\begin{eqnarray}
k(x, x^\prime) = \exp(- \Vert x - x^\prime \Vert^2 / 2 \ell^2 ) \nonumber
\end{eqnarray}\]

<p>where \(\ell\) is the lengthscale parameter.</p>

<p><strong>Matérn kernel.</strong></p>

\[\begin{eqnarray}
k(x, x^\prime) = (2^{1 - \nu} / \Gamma(\nu)) r^\nu B_\nu(r), \quad r = (\sqrt{2 \nu} / \ell) \Vert x - x^\prime \Vert \nonumber
\end{eqnarray}\]

<p>where \(B_\nu\) is the modified Bessel function. \(\nu\) controls the smoothness of sample paths</p>

<hr>

<p><strong>Theorem 5</strong> <em>Let \(D \in \mathbb{R}^d\) be compact and convex, \(d \in \mathbb{N}\). Assume the kernel \(k(x, x^\prime) \leq 1\).</em></p>

<ol>
  <li><em>Finite spectrum. For the \(d\)-dimensional Bayesian linear regression case: \(\gamma_T = \mathcal{O}(d \log T)\).</em></li>
  <li><em>Exponential spectral decay. For the squared exponential kernel: \(\mathcal{O}((\log T)^{d + 1})\).</em></li>
  <li><em>Power law spectral decay. For the Matérn kernels with \(\nu &gt; 1\): \(\mathcal{O}(T^{d(d + 1) / (2 \nu + d (d + 1))})\).</em></li>
</ol>

<p>Sketch of proof:</p>

<p>\(\gamma_T\) is bounded by <strong>Theorem 4</strong> (See <a href="https://marshalarijona.github.io/blog/2024/GP-UCB-part2/">Part II</a>) in terms the eigendecay of the kernel matrix \(\mathbf{K}_D\). If \(D\) is infinite or very large, we can use the operator spectrum of \(k(x, x_0)\), which likewise decays rapidly. For the kernels of interest here, there exist asymptotic expressions for the operator eigenvalues. The key of the proof is to show the existence of discretization \(D_T \subset D\), dense in the limit, for which the tail sum \(B(T_\ast) / n_T\) in <strong>Theorem 4</strong> are close to corresponding operator.</p>

<h4 id="reference">Reference</h4>

<ul>
  <li>Srinivas, N., Krause, A., Kakade, S. M., &amp; Seeger, M. (2009). Gaussian process optimization in the bandit setting: No regret and experimental design. arXiv preprint arXiv:0912.3995.</li>
</ul>

  </article>

</div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2024 Marshal Arijona Sinaga. Powered by <a href="http://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Mansory & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/mansory.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
  </body>
</html>

