<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>        
    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Marshal Sinaga | Publications</title>
    <meta name="author" content="Marshal Arijona Sinaga" />
    <meta name="description" content="publications by chronological order." />
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://gitcdn.xyz/repo/jwarby/jekyll-pygments-themes/master/github.css" />

    <!-- Styles -->
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22></text></svg>">
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://marshalarijona.github.io/publications/">

    <!-- Dark Mode -->
    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
  </head>

  <!-- Body -->
  <body class="fixed-top-nav">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="https://marshalarijona.github.io/">Marshal Sinaga</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">About</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item ">
                <a class="nav-link" href="/blog/">Blog</a>
              </li>

              <!-- Other pages-->
              <li class="nav-item ">
                <a class="nav-link" href="/notes/">Notes</a>
              </li>
              <li class="nav-item active">
                <a class="nav-link" href="/publications/">Publications<span class="sr-only">(current)</span></a>
              </li>
              <!--
              <li class="nav-item dropdown ">
                <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus</a>
                <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown">
                  <a class="dropdown-item" href="/publications/">publications</a>
                  <div class="dropdown-divider"></div>
                  <a class="dropdown-item" href="/projects/">projects</a>
                </div>
              </li>
            -->

              <!-- Toogle theme mode -->
              <div class="toggle-container">
                <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </a>
              </div>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <!-- page.html -->
        <div class="post">

          <header class="post-header">
            <h1 class="post-title">Publications</h1>
            <p class="post-description">publications by chronological order.</p>
          </header>

          <article>
            <!-- _pages/publications.md -->
<div class="publications">
  <h2 class="year">2020</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ICACSIS 2020</abbr></div>

        <!-- Entry bib key -->
        <div id="sinaga2020least" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Least Square Adversarial Autoencoder</div>
          <!-- Author -->
          <div class="author">
                  <em>Sinaga, Marshal</em>, and Stefanus, Lim Yohanes
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>2020 International Conference on Advanced Computer Science and Information Systems (ICACSIS)</em> 2020
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Cite</a>
            <a href="https://www.researchgate.net/publication/344516945_Least_Square_Adversarial_Autoencoder" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a>
            <a href="https://github.com/MarshalArijona/least-square-adversarial-autoencoder" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>This research introduces least square adversarial autoencoder (LSAA)-an autoencoder that is able to reconstruct data and also generate data that has characteristics similar to data distribution from the prior distribution. LSAA uses least square generative adversarial network loss function on its discriminator. LSAA minimizes Pearson χ 2 divergence between the latent variable distribution and the prior distribution. In this research, a Python program is developed to model LSAA by utilizing MNIST data set and FashionMNIST data set. The program is implemented using PyTorch. All of the programming activities are carried out in the cloud environment provided by the Tokopedia-Universitas Indonesia AI Center, using DGX-1 (GPU Tesla V100) as its computing resource. The experimental results show that the mean squared error of LSAA for MNIST data set and FashionMNIST data set are 0.0080 and 0.0099, respectively. Furthermore, the Fréchet Inception Distance score of LSAA for MNIST data set and FashionMNIST data set are 11.1280 and 27.5737, respectively. These results indicate that the least square adversarial autoencoder is able to reconstruct the image properly and also able to generate images similar to the training samples.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">sinaga2020least</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Least Square Adversarial Autoencoder}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sinaga, Marshal and Stefanus, Lim Yohanes}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{2020 International Conference on Advanced Computer Science and Information Systems (ICACSIS)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{33--40}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE,}</span><span class="p">,</span>
  <span class="na">paper</span> <span class="p">=</span> <span class="s">{https://www.researchgate.net/publication/344516945_Least_Square_Adversarial_Autoencoder}</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{ICACSIS 2020}</span><span class="p">,</span>
  <span class="na">code</span> <span class="p">=</span> <span class="s">{https://github.com/MarshalArijona/least-square-adversarial-autoencoder}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"></div>

        <!-- Entry bib key -->
        <div id="sinaga2020thesis" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Bachelor’s thesis: Least Square Adversarial Autoencoder and Its Application for
Image Reconstruction and Image Generation (in Bahasa Indonesia)</div>
          <!-- Author -->
          <div class="author">
                <em>Sinaga, Marshal</em>
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em></em> 2020
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="/assets/pdf/thesis/bachelor_thesis.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>This Final Project (Tugas Akhir) investigates the least square adversarial autoencoder that
uses least square generative adversarial network as its discriminator. The discriminator
minimizes the Pearson χ2 divergence between the latent variable distribution and the
prior distribution. The presence of discriminator allows the autoencoder to generate
data that has characteristics that resemble the original data. Python programs were
developed to model the least square adversarial autoencoder. This programs try to model
two types of autoencoder namely unsupervised least square adversarial autoencoder
and supervised least square adversarial autoencoder by utilizing MNIST dataset and
FashionMNIST dataset. The unsupervised least square adversarial autoencoder uses
latent variables of dimension 20 while the supervised least square adversarial autoencoder
uses latent variables with dimensions of 2, 3, 4, and 5, respectively. This programs
were implemented using PyTorch and executed using Jupyter Notebook. All of the
programming activities are carried out in the cloud environment provided by Floydhub
and Tokopedia-UI AI Center, respectively using NVIDIA Tesla K80 GPU and NVIDIA
Tesla V100 GPU as their computing resource. Training time in unsupervised least
square adversarial autoencoder lasts for two hours while in supervised least square
adversarial autoencoder lasts for six hours. The Results of experiments show that the
mean squared error of unsupervised least square adversarial autoencoder for MNIST
dataset and FashionMNIST dataset are 0.0063 and 0.0094, respectively. Meanwhile,
the mean squared error of supervised least square adversarial autoencoder for MNIST
dataset is 0.0033. Furthermore, the Fr ́echet Inception Distance scores of unsupervised
least square adversarial autoencoder for MNIST dataset and FashionMNIST dataset are
15.7182 and 38.6967, respectively. Meanwhile, the value of Fr ́echet Inception Distance
score of supervised least square adversarial autoencoder in MNIST dataset is 62.512.
These results indicate that the least square adversarial autoencoder is able to reconstruct
the image properly, but is less able to generate images with the same quality as the
learning sample.</p>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2021</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"></div>

        <!-- Entry bib key -->
        <div id="sinaga2021thesis" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Master’s Thesis: Transformation-Equivariant Representation Learning with Barber-Agakov and Noise Contrastive Mutual Information Estimation (in Bahasa Indonesia)</div>
          <!-- Author -->
          <div class="author">
                <em>Sinaga, Marshal</em>
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em></em> 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="/assets/pdf/thesis/master_thesis.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Convolution neural network (CNN) has shown promising results on various
image classification tasks. One of the reasons due to the ability of CNN
to extract representation that is equivariant to transformations. However, the
notion only holds for the translation transformation. This research introduces
variational transformation equivariant (VTE), a more general unsupervised
transformation-equivariant representation model. During the implementation,
VTE utilizes the Predictive-transformation, a self-supervised learning model that
acts as inductive bias. The optimization of VTE involves two lower bound
mutual information methods: Barber-Agakov and information noise contrastive
(InfoNCE). The VTE models are evaluated based on the average error rate
on image classification tasks on CIFAR-10 and STL-10 datasets. We utilize
multi-layer perceptron, K-nearest neighbor, and multinomial logistic regression as
the classifiers. Results show VTE with Barber-Agakov and VTE with InfoNCE
outperform the baseline model for each classifier on both datasets. Specifically,
VTEBA consistently achieves the lowest average error rate for both datasets.
</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">IWBIS 2021</abbr></div>

        <!-- Entry bib key -->
        <div id="sinaga2021club" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Variational Contrastive Log Ratio Upper Bound of Mutual Information for Training Generative Models</div>
          <!-- Author -->
          <div class="author">
                  <em>Sinaga, Marshal</em>, Alhamidi, Machmud Roby, Rachmadi, Muhammad Febrian, and Jatmiko, Wisnu
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>2021 6th International Workshop on Big Data and Information Security (IWBIS)</em> 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Cite</a>
            <a href="https://www.researchgate.net/publication/349881819_Variational_Contrastive_Log_Ratio_Upper_Bound_of_Mutual_Information_for_Training_Generative_Model" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a>
            <a href="https://github.com/MarshalArijona/CLUB-Generative-Network" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Theoretically, a Generative adversarial network minimizes the Jensen-Shannon divergence between real data distribution and generated data distribution. This divergence is another form of mutual information between a mixture distribution and a binary distribution. It implies that we can build a similar generative model by optimizing the mutual information. This research proposes variational contrastive log-ratio upper bound vCLUB mutual information estimation on mixture distribution and the optimization algorithm to train two generative models. We call the models CLUB-sampling generative network (vCLUBsampling GN) and vCLUB-non sampling generative network (vCLUB-non sampling GN). The results show that vCLUBsampling outperforms GAN and vCLUB-non sampling GN on the MNIST dataset and has competitive results with GAN on the CIFAR-10 dataset. However, GAN outperforms vCLUB-non sampling GN on both datasets.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">sinaga2021club</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Variational Contrastive Log Ratio Upper Bound of Mutual Information for Training Generative Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sinaga, Marshal and Alhamidi, Machmud Roby and Rachmadi, Muhammad Febrian and Jatmiko, Wisnu}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{2021 6th International Workshop on Big Data and Information Security (IWBIS)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{9--16}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE,}</span><span class="p">,</span>
  <span class="na">code</span> <span class="p">=</span> <span class="s">{https://github.com/MarshalArijona/CLUB-Generative-Network}</span><span class="p">,</span>
  <span class="na">paper</span> <span class="p">=</span> <span class="s">{https://www.researchgate.net/publication/349881819_Variational_Contrastive_Log_Ratio_Upper_Bound_of_Mutual_Information_for_Training_Generative_Model}</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{IWBIS 2021}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ICONIP 2021</abbr></div>

        <!-- Entry bib key -->
        <div id="sinaga2021tile2vec" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Tile2Vec with Predicting Noise for Land Cover Classification</div>
          <!-- Author -->
          <div class="author">
                  <em>Sinaga, Marshal</em>, Ali, Fadel Muhammad, and Arymurthy, Aniati Murni
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>International Conference on Neural Information Processing</em> 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Cite</a>
            <a href="https://www.researchgate.net/publication/356458961_Tile2Vec_with_Predicting_Noise_for_Land_Cover_Classification" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a>
            <a href="https://github.com/MarshalArijona/tile2vec-with-predicting-noise/tree/master" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Tile2vec has proven to be a good representation learning model in the remote sensing field. The success of the model depends on l2-norm regularization. However, l2-norm regularization has the main drawback that affects the regularization. We propose to replace the l2-norm with regularization with predicting noise framework. We then develop an algorithm to integrate the framework. We evaluate the model by using it as a feature extractor on the land cover classification task. The result shows that our proposed model outperforms all the baseline models.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">sinaga2021tile2vec</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Tile2Vec with Predicting Noise for Land Cover Classification}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sinaga, Marshal and Ali, Fadel Muhammad and Arymurthy, Aniati Murni}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Conference on Neural Information Processing}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{87--99}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer,}</span><span class="p">,</span>
  <span class="na">paper</span> <span class="p">=</span> <span class="s">{https://www.researchgate.net/publication/356458961_Tile2Vec_with_Predicting_Noise_for_Land_Cover_Classification}</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{ICONIP 2021}</span><span class="p">,</span>
  <span class="na">code</span> <span class="p">=</span> <span class="s">{https://github.com/MarshalArijona/tile2vec-with-predicting-noise/tree/master}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"></div>

        <!-- Entry bib key -->
        <div id="sinaga2021variational" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">On study of Variational Inference</div>
          <!-- Author -->
          <div class="author">
                <em>Sinaga, Marshal</em>
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em></em> 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="approximate_inference.pdf" class="btn btn-sm z-depth-0" role="button">Paper</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The main problem of Bayesian inference is computing the posterior which is often intractable. In
this paper, we review variational inference (VI)
methods that aim to find a variational distribution to approximate the true posterior. The review
starts with the original form of variational inference. Then we also discuss the extension of VI:
mean-field, stochastic and black-box variational,
and amortized inference. Finally, we review the
recent improvement of variational inference.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"></div>

        <!-- Entry bib key -->
        <div id="sinaga2021MI" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">On Study of Mutual Information and Its Estimation
Methods</div>
          <!-- Author -->
          <div class="author">
                <em>Sinaga, Marshal</em>
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em></em> 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="mutual_information.pdf" class="btn btn-sm z-depth-0" role="button">Paper</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The presence of mutual information in the research
of deep learning has grown significantly. It has been proven
that mutual information can be a good objective function to
build a robust deep learning model. Most of the researches
utilize estimation methods to approximate the true mutual
information. This technical report delivers an extensive study
about definitions as well as properties of mutual information.
This article then delivers some reviews and current drawbacks
of mutual information estimation methods afterward.</p>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2022</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ICPRAM 2022</abbr></div>

        <!-- Entry bib key -->
        <div id="sinaga2022vte" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Transformation Equivariant Representation Learning with Barber Agakov and Info-NCE Mutual Information Estimation</div>
          <!-- Author -->
          <div class="author">
                  <em>Sinaga, Marshal</em>, Basarrudin, T., and Krisnadhi, Adila Alfa
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>International Conference on Pattern Recognition Applications and Methods (ICPRAM)</em> 2022
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Cite</a>
            <a href="https://www.researchgate.net/publication/356459026_Transformation_Equivariant_Representation_Learning_with_Barber_Agakov_and_Info-NCE_Mutual_Information_Estimation" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a>
            <a href="https://github.com/MarshalArijona/VTE/tree/master" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The success of deep learning on computer vision tasks is due to the convolution layer that equivaries to the translation transformation. Several works attempt to extend the notion of equivariance into more general transformations. Autoencoding variational transformation (AVT) achieves state of art by approaching the problem from the information theory perspective. The model involves the computation of mutual information, which leads to a more general transformation equivariant representation model. In this research, we investigate the alternatives of AVT called variational transformation equivariant (VTE). We utilize the Barber-Agakov and Info-NCE mutual information estimation to optimize VTE. Furthermore, we also propose a sequential mechanism to train our VTE. Results of experiments demonstrate that VTE outperforms AVT on image classification tasks.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">sinaga2022vte</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Transformation Equivariant Representation Learning with Barber Agakov and Info-NCE Mutual Information Estimation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sinaga, Marshal and Basarrudin, T. and Krisnadhi, Adila Alfa}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Conference on Pattern Recognition Applications and Methods (ICPRAM)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Scitepress,}</span><span class="p">,</span>
  <span class="na">paper</span> <span class="p">=</span> <span class="s">{https://www.researchgate.net/publication/356459026_Transformation_Equivariant_Representation_Learning_with_Barber_Agakov_and_Info-NCE_Mutual_Information_Estimation}</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{ICPRAM 2022}</span><span class="p">,</span>
  <span class="na">code</span> <span class="p">=</span> <span class="s">{https://github.com/MarshalArijona/VTE/tree/master}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li></ol>

  <h2 class="year">2023</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">RealML 2023</abbr></div>

        <!-- Entry bib key -->
        <div id="sinaga2023heteroPBO" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Preferential Heteroscedastic Bayesian Optimization with
Informative Noise Priors</div>
          <!-- Author -->
          <div class="author">
                  <em>Sinaga, Marshal</em>, Martinelli, Julien, and Kaski, Samuel
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>NeurIPS 2023 Workshop on Adaptive Experimental Design and Active Learning in the Real World</em> 2023
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="/assets/pdf/heteroscedasticPBO_neurips2023.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Preferential Bayesian optimization (PBO) is a sample-efficient framework for optimizing
a black-box function by utilizing human preferences between two candidate solutions as a
proxy. Conventional PBO relies on homoscedastic noise to model human preference struc-
ture. However, such noise fails to accurately capture the varying levels of human aleatoric
uncertainty among different pairs of candidates. For instance, a chemist with solid expertise
in glucose-related molecules may easily compare two compounds and struggle for alcohol-
related molecules. Furthermore, PBO ignores this uncertainty when searching for a new
candidate, consequently underestimating the risk associated with human uncertainty. To
address this, we propose heteroscedastic noise models to learn human preference structure.
Moreover, we integrate the preference structure with the acquisition functions that account
for aleatoric uncertainty. The noise models assign noise based on the distance of a specific
input to a predefined set of reliable inputs known as anchors. We empirically evaluate the
proposed approach on a range of synthetic black-box functions, demonstrating a consistent
improvement over homoscedastic PBO.</p>
          </div>
        </div>
      </div>
</li></ol>

  <h2 class="year">2024</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">preprint</abbr></div>

        <!-- Entry bib key -->
        <div id="sinaga2024heteroPBO" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Preferential Heteroscedastic Bayesian Optimization with
Informative Noise Distribution</div>
          <!-- Author -->
          <div class="author">
                  <em>Sinaga, Marshal</em>, Martinelli, Julien, Garg, Vikas, and Kaski, Samuel
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em></em> 2024
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://arxiv.org/abs/2405.14657" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Preferential Bayesian optimization (PBO) is a sample-efficient framework for
learning human preferences between candidate designs. PBO classically relies on
homoscedastic noise models to represent human aleatoric uncertainty. Yet, such
noise fails to accurately capture the varying levels of human aleatoric uncertainty,
particularly when the user possesses partial knowledge among different pairs
of candidates. For instance, a chemist with solid expertise in glucose-related
molecules may easily compare two compounds from that family while struggling
to compare alcohol-related molecules. Currently, PBO overlooks this uncertainty
during the search for a new candidate through the maximization of the acquisition
function, consequently underestimating the risk associated with human uncertainty.
To address this issue, we propose a heteroscedastic noise model to capture human
aleatoric uncertainty. This model adaptively assigns noise levels based on the
distance of a specific input to a predefined set of reliable inputs known as anchors
provided by the human. Anchors encapsulate partial knowledge and offer insight
into the comparative difficulty of evaluating different candidate pairs. Such a model
can be seamlessly integrated into the acquisition function, thus leading to candidate
design pairs that elegantly trade informativeness and ease of comparison for the
human expert. We perform an extensive empirical evaluation of the proposed
approach, demonstrating a consistent improvement over homoscedastic PBO.</p>
          </div>
        </div>
      </div>
</li></ol>


</div>

          </article>

        </div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2024 Marshal Arijona Sinaga. Powered by <a href="http://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Mansory & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/mansory.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
  </body>
</html>

